{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e632aa93",
   "metadata": {},
   "source": [
    "# *RAG Implementation Fake News Detection:*\n",
    "### *Aman Pawar*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d54c6531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiprocessing start method set to 'spawn' (was None).\n",
      "CUDA available. Using device: cuda\n",
      "\n",
      "--- Setting up Retriever ---\n",
      "Loaded 6420 samples from ../Constraint_English_Train.xlsx\n",
      "Loading retriever model: all-MiniLM-L6-v2...\n",
      "Retriever model loaded.\n",
      "Embedding 6420 training documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0319e7345bdc4865b898198a6706cee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([6420, 384])\n",
      "Building FAISS index (IndexFlatL2) with dimension 384...\n",
      "FAISS index built with 6420 vectors.\n",
      "\n",
      "--- Loading Data and Creating DataLoaders ---\n",
      "Loaded 2140 samples from ../Constraint_English_Val.xlsx\n",
      "Loaded 2140 samples from ../english_test_with_labels.xlsx\n",
      "Loading classifier tokenizer: bert-base-uncased...\n",
      "Classifier tokenizer loaded.\n",
      "Creating datasets...\n",
      "Datasets created.\n",
      "DEBUG MODE: Setting num_workers = 0 to get detailed error tracebacks.\n",
      "pin_memory set to: False\n",
      "DataLoaders created.\n",
      "  Train batches: 803\n",
      "  Validation batches: 268\n",
      "  Test batches: 268\n",
      "\n",
      "--- Initializing Classifier Model ---\n",
      "Loading classifier model: bert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier model loaded and moved to device.\n",
      "\n",
      "--- Setting up Training Components ---\n",
      "Optimizer, Scheduler, and Loss Function initialized.\n",
      "Total training steps (considering grad accum): 2409\n",
      "\n",
      "--- Starting Training ---\n",
      "\n",
      "===== Epoch 1/3 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15d5180960a4af186fd2fb000a2f46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 Training:   0%|          | 0/803 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 completed in 205.53 seconds.\n",
      "  Average Training Loss: 0.2429\n",
      "\n",
      "--- Evaluating on Validation Set ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3acda98c534f35884d45940f10af29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "  Loss: 0.2876\n",
      "  Accuracy: 0.9336\n",
      "  Precision: 0.9757\n",
      "  Recall: 0.8955\n",
      "  F1-Score: 0.9339\n",
      "\n",
      "Validation F1 improved (-1.0000 --> 0.9339). Saving model...\n",
      "Model saved to ./rag_classifier_model\n",
      "\n",
      "===== Epoch 2/3 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce0a0980ada4bbb883be18bfd0ba65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 Training:   0%|          | 0/803 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 completed in 205.83 seconds.\n",
      "  Average Training Loss: 0.0447\n",
      "\n",
      "--- Evaluating on Validation Set ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167c7a7c5fbc49f88bb2e7570801d64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "  Loss: 0.1881\n",
      "  Accuracy: 0.9551\n",
      "  Precision: 0.9697\n",
      "  Recall: 0.9437\n",
      "  F1-Score: 0.9566\n",
      "\n",
      "Validation F1 improved (0.9339 --> 0.9566). Saving model...\n",
      "Model saved to ./rag_classifier_model\n",
      "\n",
      "===== Epoch 3/3 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f6873af591446fa811587e63c2c0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 Training:   0%|          | 0/803 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 completed in 227.76 seconds.\n",
      "  Average Training Loss: 0.0092\n",
      "\n",
      "--- Evaluating on Validation Set ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522c0eff1b2246b8bb140b6c8a3775f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "  Loss: 0.1993\n",
      "  Accuracy: 0.9584\n",
      "  Precision: 0.9640\n",
      "  Recall: 0.9563\n",
      "  F1-Score: 0.9601\n",
      "\n",
      "Validation F1 improved (0.9566 --> 0.9601). Saving model...\n",
      "Model saved to ./rag_classifier_model\n",
      "\n",
      "--- Training Finished ---\n",
      "\n",
      "--- Evaluating on Test Set using the Best Model ---\n",
      "No saved best model found. Evaluating using the model's final state...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1cae619cb84648ac90d60b6d2ee3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "  Loss: 0.2083\n",
      "  Accuracy: 0.9617\n",
      "  Precision: 0.9634\n",
      "  Recall: 0.9634\n",
      "  F1-Score: 0.9634\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.multiprocessing as mp # Import torch multiprocessing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss # For efficient similarity search\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import time\n",
    "import gc # Garbage collector\n",
    "import traceback # For detailed error printing\n",
    "\n",
    "# --- Set Environment Variable to Suppress Tokenizer Parallelism Warning ---\n",
    "# This should be done early, before tokenizers are potentially used by multiprocessing workers\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "class Config:\n",
    "    \"\"\"Configuration class for hyperparameters and settings.\"\"\"\n",
    "    # File paths (Update these paths if your files are located elsewhere)\n",
    "    train_file = '../Constraint_English_Train.xlsx'\n",
    "    val_file = '../Constraint_English_Val.xlsx'\n",
    "    test_file = '../english_test_with_labels.xlsx'\n",
    "\n",
    "    # Model names\n",
    "    retriever_model_name = 'all-MiniLM-L6-v2' # Efficient sentence transformer for retrieval\n",
    "    classifier_model_name = 'bert-base-uncased' # Base model for classification ('roberta-base', etc.)\n",
    "\n",
    "    # RAG parameters\n",
    "    num_retrieved_docs = 3 # Number of relevant documents to retrieve per input\n",
    "\n",
    "    # Training parameters\n",
    "    max_seq_length = 512 # Max token length for combined input (query + retrieved docs)\n",
    "    batch_size = 8       # Adjust based on GPU memory\n",
    "    epochs = 3           # Number of training epochs\n",
    "    learning_rate = 2e-5 # Learning rate for the optimizer\n",
    "    warmup_steps = 100   # Number of warmup steps for the scheduler\n",
    "    gradient_accumulation_steps = 1 # Increase if batch size needs to be effectively larger due to memory limits\n",
    "\n",
    "    # Hardware and reproducibility\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    seed = 42\n",
    "\n",
    "    # Output directory for saving the best model\n",
    "    output_dir = \"./rag_classifier_model\"\n",
    "\n",
    "    # --- DEBUGGING FLAG ---\n",
    "    # Set to True to force num_workers=0 and get better error tracebacks\n",
    "    # Set back to False for normal parallel data loading\n",
    "    DEBUG_DATALOADER = True # <-- SET THIS TO True FOR DEBUGGING WORKER ERRORS\n",
    "\n",
    "# --- 2. Data Loading Function ---\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Loads data from an Excel file, performs basic cleaning, and maps labels.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the Excel file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Loaded and preprocessed data, or None if loading fails.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Data file not found: {filepath}\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_excel(filepath)\n",
    "\n",
    "        # Verify essential columns exist\n",
    "        if 'tweet' not in df.columns or 'label' not in df.columns:\n",
    "             print(f\"Error: Excel file {filepath} must contain 'tweet' and 'label' columns.\")\n",
    "             return None\n",
    "\n",
    "        # Drop rows where 'tweet' is NaN and ensure 'tweet' is string type\n",
    "        df = df.dropna(subset=['tweet'])\n",
    "        df['tweet'] = df['tweet'].astype(str)\n",
    "\n",
    "        # Map labels to integers ('real': 1, 'fake': 0)\n",
    "        df['label'] = df['label'].map({'real': 1, 'fake': 0})\n",
    "\n",
    "        # Drop rows where label mapping failed (i.e., label was not 'real' or 'fake')\n",
    "        df = df.dropna(subset=['label'])\n",
    "        df['label'] = df['label'].astype(int)\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"Warning: No valid data loaded from {filepath} after cleaning.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Loaded {len(df)} samples from {filepath}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        traceback.print_exc() # Print detailed traceback\n",
    "        return None\n",
    "\n",
    "# --- 3. Custom PyTorch Dataset for RAG ---\n",
    "class RagNewsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that retrieves relevant documents using a retriever function\n",
    "    and combines them with the original text for input to the classifier.\n",
    "    Includes enhanced error handling in __getitem__.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_len, retriever_func, num_retrieved):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.retriever_func = retriever_func\n",
    "        self.num_retrieved = num_retrieved\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an item, finds context, tokenizes, and returns dict.\n",
    "        Includes specific try-except blocks for debugging worker errors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            original_text = str(self.texts[idx])\n",
    "            label = int(self.labels[idx])\n",
    "        except Exception as e:\n",
    "            print(f\"DataLoader Worker Error: Failed to get text/label at index {idx}. Error: {e}\")\n",
    "            # Option 1: Return None or raise an error to stop (if num_workers=0)\n",
    "            # Option 2: Return a dummy item (might hide errors but allow continuation)\n",
    "            # For debugging, raising is better if num_workers=0\n",
    "            if mp.current_process().daemon: # Check if in a worker process\n",
    "                 print(f\"Problematic text (first 100 chars): {str(self.texts[idx])[:100] if idx < len(self.texts) else 'Index out of bounds'}\")\n",
    "                 # Workers should ideally not raise exceptions that crash them,\n",
    "                 # but returning None might cause issues downstream in collate_fn.\n",
    "                 # It's often better to fix the root cause found when num_workers=0.\n",
    "                 # For now, let it potentially crash the worker if data access fails.\n",
    "                 pass # Or implement dummy return if needed, but not ideal for finding errors\n",
    "            raise e # Re-raise if in main process (num_workers=0) or let worker crash\n",
    "\n",
    "        # --- Retrieval Step ---\n",
    "        retrieved_docs = []\n",
    "        try:\n",
    "            retrieved_docs = self.retriever_func(original_text, k=self.num_retrieved)\n",
    "        except Exception as e:\n",
    "            print(f\"DataLoader Worker Error: Failed during RETRIEVAL for index {idx}. Error: {e}\")\n",
    "            # Print part of the text that caused the retrieval error\n",
    "            print(f\"  Query text (first 100 chars): {original_text[:100]}\")\n",
    "            # Fallback to empty context, but the error is logged\n",
    "            retrieved_docs = []\n",
    "            # If debugging with num_workers=0, you might want to raise e here too\n",
    "\n",
    "        # --- Tokenization Step ---\n",
    "        try:\n",
    "            context = \" \".join(retrieved_docs)\n",
    "            combined_text = f\"{original_text} [SEP] {context}\"\n",
    "\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                combined_text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"DataLoader Worker Error: Failed during TOKENIZATION for index {idx}. Error: {e}\")\n",
    "            # Print part of the text that caused the tokenization error\n",
    "            print(f\"  Combined text (first 100 chars): {combined_text[:100]}\")\n",
    "            # If tokenization fails, we cannot return valid tensors.\n",
    "            # Raising the error is best when num_workers=0.\n",
    "            # If in a worker, crashing might be unavoidable if we can't produce valid output.\n",
    "            if not mp.current_process().daemon:\n",
    "                 raise e # Re-raise if in main process\n",
    "            else:\n",
    "                 # What to do in a worker? Returning None often breaks the collate_fn.\n",
    "                 # Crashing the worker might be the only outcome, leading back to the original error.\n",
    "                 # This highlights why num_workers=0 is key for debugging this stage.\n",
    "                 print(\"FATAL: Cannot proceed with tokenization error in worker.\")\n",
    "                 # To prevent infinite loops, maybe return a dummy tensor of the correct shape?\n",
    "                 # This is complex and error-prone. Best to fix the root cause.\n",
    "                 # For now, let it potentially crash.\n",
    "                 pass\n",
    "\n",
    "\n",
    "        # --- Return Result ---\n",
    "        try:\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        except Exception as e:\n",
    "             print(f\"DataLoader Worker Error: Failed during final DICT CREATION for index {idx}. Error: {e}\")\n",
    "             # This should be rare if encoding succeeded.\n",
    "             if not mp.current_process().daemon:\n",
    "                  raise e\n",
    "             else:\n",
    "                  # Let worker crash if it can't form the final dict\n",
    "                  pass\n",
    "\n",
    "\n",
    "# --- 4. Evaluation Function ---\n",
    "# (Keep evaluate_model function as is)\n",
    "def evaluate_model(model, dataloader, device, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"  Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- Multiprocessing Setup ---\n",
    "    # (Keep multiprocessing setup as is)\n",
    "    try:\n",
    "        current_start_method = mp.get_start_method(allow_none=True)\n",
    "        if current_start_method != 'spawn':\n",
    "             mp.set_start_method('spawn', force=True)\n",
    "             print(f\"Multiprocessing start method set to 'spawn' (was {current_start_method}).\")\n",
    "        else:\n",
    "            print(\"Multiprocessing start method already set to 'spawn'.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Note: Could not set multiprocessing start method to 'spawn': {e}\")\n",
    "    except AttributeError:\n",
    "        print(\"Note: Unable to check/set multiprocessing start method (possibly older Python/torch version).\")\n",
    "\n",
    "\n",
    "    # --- Seed and Device Setup ---\n",
    "    # (Keep seed and device setup as is)\n",
    "    np.random.seed(Config.seed)\n",
    "    torch.manual_seed(Config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(Config.seed)\n",
    "        print(f\"CUDA available. Using device: {Config.device}\")\n",
    "    else:\n",
    "        print(f\"CUDA not available. Using device: {Config.device}\")\n",
    "\n",
    "    # --- Retriever Setup ---\n",
    "    # (Keep retriever setup as is)\n",
    "    print(\"\\n--- Setting up Retriever ---\")\n",
    "    train_df = None; train_texts = []; retriever_model = None; faiss_index = None\n",
    "    try:\n",
    "        train_df = load_data(Config.train_file)\n",
    "        if train_df is None: raise ValueError(\"Failed to load training data for retriever.\")\n",
    "        train_texts = train_df['tweet'].tolist()\n",
    "        print(f\"Loading retriever model: {Config.retriever_model_name}...\")\n",
    "        retriever_model = SentenceTransformer(Config.retriever_model_name, device=Config.device)\n",
    "        print(\"Retriever model loaded.\")\n",
    "        print(f\"Embedding {len(train_texts)} training documents...\")\n",
    "        batch_size_embed = 64\n",
    "        train_embeddings = retriever_model.encode(train_texts, batch_size=batch_size_embed, convert_to_tensor=True, show_progress_bar=True, device=Config.device)\n",
    "        print(f\"Embeddings shape: {train_embeddings.shape}\")\n",
    "        train_embeddings_cpu = train_embeddings.cpu().numpy().astype(np.float32)\n",
    "        del train_embeddings; gc.collect()\n",
    "        if Config.device == torch.device(\"cuda\"): torch.cuda.empty_cache()\n",
    "        embedding_dim = train_embeddings_cpu.shape[1]\n",
    "        print(f\"Building FAISS index (IndexFlatL2) with dimension {embedding_dim}...\")\n",
    "        faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "        faiss_index.add(train_embeddings_cpu)\n",
    "        print(f\"FAISS index built with {faiss_index.ntotal} vectors.\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Error during retriever setup: {e}\"); traceback.print_exc(); exit(1)\n",
    "\n",
    "    # --- Retrieval Function Definition ---\n",
    "    # (Keep retrieve_documents function as is)\n",
    "    def retrieve_documents(query_text, k=Config.num_retrieved_docs):\n",
    "        if retriever_model is None or faiss_index is None: return []\n",
    "        try:\n",
    "            query_embedding = retriever_model.encode([query_text], convert_to_tensor=True, device=Config.device)\n",
    "            query_embedding_np = query_embedding.cpu().numpy().astype(np.float32)\n",
    "            distances, indices = faiss_index.search(query_embedding_np, k)\n",
    "            retrieved_texts = []\n",
    "            for i in indices[0]:\n",
    "                if 0 <= i < len(train_texts): retrieved_texts.append(train_texts[i])\n",
    "                else: print(f\"Warning: Retrieved invalid index {i} during search.\")\n",
    "            return retrieved_texts\n",
    "        except Exception as e:\n",
    "             print(f\"ERROR in retrieve_documents for query '{query_text[:50]}...': {e}\")\n",
    "             return []\n",
    "\n",
    "\n",
    "    # --- Load Data & Create DataLoaders ---\n",
    "    print(\"\\n--- Loading Data and Creating DataLoaders ---\")\n",
    "    val_df, test_df = None, None\n",
    "    train_dataset, val_dataset, test_dataset = None, None, None\n",
    "    train_dataloader, val_dataloader, test_dataloader = None, None, None\n",
    "    classifier_tokenizer = None\n",
    "\n",
    "    try:\n",
    "        # Load validation and test data\n",
    "        val_df = load_data(Config.val_file)\n",
    "        test_df = load_data(Config.test_file)\n",
    "        if val_df is None or test_df is None: raise ValueError(\"Failed to load validation or test data.\")\n",
    "\n",
    "        # Initialize tokenizer\n",
    "        print(f\"Loading classifier tokenizer: {Config.classifier_model_name}...\")\n",
    "        classifier_tokenizer = AutoTokenizer.from_pretrained(Config.classifier_model_name)\n",
    "        print(\"Classifier tokenizer loaded.\")\n",
    "\n",
    "        # Create Datasets\n",
    "        print(\"Creating datasets...\")\n",
    "        train_dataset = RagNewsDataset(texts=train_df['tweet'].tolist(), labels=train_df['label'].tolist(), tokenizer=classifier_tokenizer, max_len=Config.max_seq_length, retriever_func=retrieve_documents, num_retrieved=Config.num_retrieved_docs)\n",
    "        val_dataset = RagNewsDataset(texts=val_df['tweet'].tolist(), labels=val_df['label'].tolist(), tokenizer=classifier_tokenizer, max_len=Config.max_seq_length, retriever_func=retrieve_documents, num_retrieved=Config.num_retrieved_docs)\n",
    "        test_dataset = RagNewsDataset(texts=test_df['tweet'].tolist(), labels=test_df['label'].tolist(), tokenizer=classifier_tokenizer, max_len=Config.max_seq_length, retriever_func=retrieve_documents, num_retrieved=Config.num_retrieved_docs)\n",
    "        print(\"Datasets created.\")\n",
    "\n",
    "        # --- Determine number of workers (DEBUGGING CHANGE) ---\n",
    "        if Config.DEBUG_DATALOADER:\n",
    "             num_workers = 0 # Force 0 workers for debugging\n",
    "             print(\"DEBUG MODE: Setting num_workers = 0 to get detailed error tracebacks.\")\n",
    "        else:\n",
    "             # Original logic for setting num_workers\n",
    "             num_workers = 0\n",
    "             if Config.device == torch.device(\"cuda\"):\n",
    "                  if mp.get_start_method(allow_none=True) == 'spawn':\n",
    "                       num_workers = 2 # Or your desired number\n",
    "                  else:\n",
    "                       print(\"Warning: CUDA available but start method is not 'spawn'. Using 0 dataloader workers.\")\n",
    "             print(f\"Using {num_workers} dataloader workers.\")\n",
    "        # --- End Debugging Change ---\n",
    "\n",
    "        pin_memory = (num_workers > 0 and Config.device == torch.device(\"cuda\"))\n",
    "        print(f\"pin_memory set to: {pin_memory}\")\n",
    "\n",
    "        # Create DataLoaders with the determined num_workers\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "        print(\"DataLoaders created.\")\n",
    "\n",
    "        if len(train_dataloader) == 0 or len(val_dataloader) == 0 or len(test_dataloader) == 0:\n",
    "             print(\"Warning: One or more DataLoaders are empty.\")\n",
    "        else:\n",
    "            print(f\"  Train batches: {len(train_dataloader)}\")\n",
    "            print(f\"  Validation batches: {len(val_dataloader)}\")\n",
    "            print(f\"  Test batches: {len(test_dataloader)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Error during DataLoader creation: {e}\")\n",
    "        traceback.print_exc()\n",
    "        exit(1)\n",
    "\n",
    "    # --- Model Definition ---\n",
    "    # (Keep model definition as is)\n",
    "    print(\"\\n--- Initializing Classifier Model ---\")\n",
    "    classifier_model = None\n",
    "    try:\n",
    "        print(f\"Loading classifier model: {Config.classifier_model_name}...\")\n",
    "        classifier_model = AutoModelForSequenceClassification.from_pretrained(Config.classifier_model_name, num_labels=2)\n",
    "        classifier_model.to(Config.device)\n",
    "        print(\"Classifier model loaded and moved to device.\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Failed to initialize classifier model: {e}\"); traceback.print_exc(); exit(1)\n",
    "\n",
    "\n",
    "    # --- Training Setup ---\n",
    "    # (Keep training setup as is)\n",
    "    print(\"\\n--- Setting up Training Components ---\")\n",
    "    optimizer = None; scheduler = None; loss_fn = None\n",
    "    if len(train_dataloader) > 0:\n",
    "        try:\n",
    "            optimizer = AdamW(classifier_model.parameters(), lr=Config.learning_rate)\n",
    "            total_steps = len(train_dataloader) * Config.epochs // Config.gradient_accumulation_steps\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=Config.warmup_steps, num_training_steps=total_steps)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            print(\"Optimizer, Scheduler, and Loss Function initialized.\")\n",
    "            print(f\"Total training steps (considering grad accum): {total_steps}\")\n",
    "        except Exception as e:\n",
    "            print(f\"FATAL: Error during training setup: {e}\"); traceback.print_exc(); exit(1)\n",
    "    else:\n",
    "        print(\"FATAL: Training dataloader is empty.\"); exit(1)\n",
    "\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    # (Keep training loop as is)\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    best_val_f1 = -1.0; global_step = 0\n",
    "    for epoch in range(Config.epochs):\n",
    "        print(f\"\\n===== Epoch {epoch + 1}/{Config.epochs} =====\")\n",
    "        start_time = time.time(); classifier_model.train(); total_train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1} Training\", leave=False)\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(Config.device)\n",
    "                attention_mask = batch['attention_mask'].to(Config.device)\n",
    "                labels = batch['labels'].to(Config.device)\n",
    "                outputs = classifier_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                if Config.gradient_accumulation_steps > 1: loss = loss / Config.gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "                total_train_loss += loss.item() * Config.gradient_accumulation_steps\n",
    "                if (step + 1) % Config.gradient_accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n",
    "                    torch.nn.utils.clip_grad_norm_(classifier_model.parameters(), 1.0)\n",
    "                    optimizer.step(); scheduler.step(); optimizer.zero_grad(); global_step += 1\n",
    "                    current_loss_scaled = loss.item() * Config.gradient_accumulation_steps\n",
    "                    progress_bar.set_postfix({'loss': f\"{current_loss_scaled:.4f}\"})\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError during training step {step} in epoch {epoch + 1}: {e}\"); traceback.print_exc()\n",
    "                print(\"Attempting to continue training...\"); optimizer.zero_grad(); continue\n",
    "        progress_bar.close()\n",
    "        epoch_time = time.time() - start_time\n",
    "        avg_train_loss = total_train_loss / global_step if global_step > 0 else 0\n",
    "        print(f\"\\nEpoch {epoch + 1} completed in {epoch_time:.2f} seconds.\")\n",
    "        print(f\"  Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        if len(val_dataloader) > 0:\n",
    "            print(\"\\n--- Evaluating on Validation Set ---\")\n",
    "            val_loss, val_acc, val_prec, val_rec, val_f1 = evaluate_model(classifier_model, val_dataloader, Config.device, loss_fn)\n",
    "            if val_f1 > best_val_f1:\n",
    "                print(f\"\\nValidation F1 improved ({best_val_f1:.4f} --> {val_f1:.4f}). Saving model...\")\n",
    "                best_val_f1 = val_f1; os.makedirs(Config.output_dir, exist_ok=True)\n",
    "                classifier_model.save_pretrained(Config.output_dir)\n",
    "                if classifier_tokenizer: classifier_tokenizer.save_pretrained(Config.output_dir)\n",
    "                print(f\"Model saved to {Config.output_dir}\")\n",
    "            else:\n",
    "                print(f\"\\nValidation F1 did not improve ({val_f1:.4f}). Current best: {best_val_f1:.4f}\")\n",
    "        else:\n",
    "            print(\"\\nSkipping validation: Validation dataloader is empty.\")\n",
    "        gc.collect()\n",
    "        if Config.device == torch.device(\"cuda\"): torch.cuda.empty_cache()\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "\n",
    "\n",
    "    # --- Final Evaluation on Test Set ---\n",
    "    # (Keep final evaluation as is)\n",
    "    if len(test_dataloader) > 0:\n",
    "        print(\"\\n--- Evaluating on Test Set using the Best Model ---\")\n",
    "        try:\n",
    "            best_model_path = Config.output_dir\n",
    "            if os.path.exists(best_model_path) and os.path.exists(os.path.join(best_model_path, \"pytorch_model.bin\")):\n",
    "                print(f\"Loading best model from {best_model_path}...\")\n",
    "                best_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\n",
    "                best_tokenizer = AutoTokenizer.from_pretrained(best_model_path)\n",
    "                best_model.to(Config.device)\n",
    "                print(\"Re-creating test dataset with loaded tokenizer...\")\n",
    "                test_dataset_final = RagNewsDataset(texts=test_df['tweet'].tolist(), labels=test_df['label'].tolist(), tokenizer=best_tokenizer, max_len=Config.max_seq_length, retriever_func=retrieve_documents, num_retrieved=Config.num_retrieved_docs)\n",
    "                # Determine num_workers/pin_memory for final test loader\n",
    "                num_workers_test = 0\n",
    "                if Config.DEBUG_DATALOADER: num_workers_test = 0 # Keep 0 if debugging\n",
    "                elif Config.device == torch.device(\"cuda\") and mp.get_start_method(allow_none=True) == 'spawn': num_workers_test = 2\n",
    "                pin_memory_test = (num_workers_test > 0 and Config.device == torch.device(\"cuda\"))\n",
    "                test_dataloader_final = DataLoader(test_dataset_final, batch_size=Config.batch_size, shuffle=False, num_workers=num_workers_test, pin_memory=pin_memory_test)\n",
    "                print(\"Evaluating best model on the test set...\")\n",
    "                if loss_fn: evaluate_model(best_model, test_dataloader_final, Config.device, loss_fn)\n",
    "                else: print(\"Error: Loss function not defined for final evaluation.\")\n",
    "            elif classifier_model:\n",
    "                 print(\"No saved best model found. Evaluating using the model's final state...\")\n",
    "                 if loss_fn: evaluate_model(classifier_model, test_dataloader, Config.device, loss_fn)\n",
    "                 else: print(\"Error: Loss function not defined for final evaluation.\")\n",
    "            else: print(\"Skipping final evaluation: No model available.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during final test evaluation: {e}\"); traceback.print_exc()\n",
    "    else:\n",
    "        print(\"\\nSkipping final evaluation: Test dataloader is empty.\")\n",
    "\n",
    "    print(\"\\n--- Script Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278767c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283fbe1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ce0c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
